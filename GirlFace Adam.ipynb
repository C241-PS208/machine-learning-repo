{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nlxskgLs2CoD"
      },
      "outputs": [],
      "source": [
        "# Image Size\n",
        "img_size = 224\n",
        "\n",
        "# Define the function to load and process images\n",
        "def load_and_process_images(directory, classes, img_size):\n",
        "    all_arrays = []\n",
        "    for class_num, faceshape_class in enumerate(classes):\n",
        "        path = os.path.join(directory, faceshape_class)\n",
        "        print(f\"Loading images from {path}\")\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Directory {path} does not exist.\")\n",
        "            continue\n",
        "        for img_name in os.listdir(path):\n",
        "            try:\n",
        "                img_path = os.path.join(path, img_name)\n",
        "                img_array = cv2.imread(img_path)\n",
        "                if img_array is not None:\n",
        "                    img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "                    img_array = cv2.resize(img_array, (img_size, img_size))\n",
        "                    all_arrays.append([img_array, class_num])\n",
        "                else:\n",
        "                    print(f\"Image {img_path} is None\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_name}: {e}\")\n",
        "    return np.array(all_arrays, dtype=object)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zOZOKZa72CoE"
      },
      "outputs": [],
      "source": [
        "# Define the directories and classes\n",
        "train_dir = \"C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Training\"\n",
        "test_dir = \"C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Testing\"\n",
        "classes = [\"oval\", \"rectangular\", \"round\", \"square\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading images from C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Training\\oval\n",
            "Loading images from C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Training\\rectangular\n",
            "Directory C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Training\\rectangular does not exist.\n",
            "Loading images from C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Training\\round\n",
            "Loading images from C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Training\\square\n",
            "Loading images from C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Testing\\oval\n",
            "Loading images from C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Testing\\rectangular\n",
            "Directory C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Testing\\rectangular does not exist.\n",
            "Loading images from C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Testing\\round\n",
            "Loading images from C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Testing\\square\n"
          ]
        }
      ],
      "source": [
        "# Load and process images\n",
        "train_data = load_and_process_images(train_dir, classes, img_size)\n",
        "test_data = load_and_process_images(test_dir, classes, img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and labels\n",
        "x_train = np.array([i[0] for i in train_data])\n",
        "y_train = to_categorical(np.array([i[1] for i in train_data]), num_classes=len(classes))\n",
        "x_test = np.array([i[0] for i in test_data])\n",
        "y_test = to_categorical(np.array([i[1] for i in test_data]), num_classes=len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (240, 224, 224, 3)\n",
            "y_train shape: (240, 4)\n",
            "x_test shape: (60, 224, 224, 3)\n",
            "y_test shape: (60, 4)\n"
          ]
        }
      ],
      "source": [
        "# Print shapes of datasets\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d_2  (None, 512)               0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14979396 (57.14 MB)\n",
            "Trainable params: 264708 (1.01 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "4/4 [==============================] - 53s 13s/step - loss: 4.4511 - accuracy: 0.2917 - val_loss: 1.8855 - val_accuracy: 0.4167\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 52s 13s/step - loss: 3.5305 - accuracy: 0.3458 - val_loss: 1.8036 - val_accuracy: 0.4500\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 52s 13s/step - loss: 3.4173 - accuracy: 0.3958 - val_loss: 1.8428 - val_accuracy: 0.4833\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 3.1250 - accuracy: 0.4000 - val_loss: 1.6490 - val_accuracy: 0.5167\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 52s 13s/step - loss: 3.2197 - accuracy: 0.3625 - val_loss: 1.6448 - val_accuracy: 0.4667\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 51s 14s/step - loss: 3.2614 - accuracy: 0.3542 - val_loss: 1.6193 - val_accuracy: 0.5167\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 49s 13s/step - loss: 2.9534 - accuracy: 0.4083 - val_loss: 1.3985 - val_accuracy: 0.5167\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 54s 14s/step - loss: 2.9082 - accuracy: 0.4000 - val_loss: 1.4584 - val_accuracy: 0.5167\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 2.9922 - accuracy: 0.4000 - val_loss: 1.6149 - val_accuracy: 0.5000\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 56s 15s/step - loss: 2.6694 - accuracy: 0.3917 - val_loss: 1.5046 - val_accuracy: 0.5167\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 54s 14s/step - loss: 2.6402 - accuracy: 0.4375 - val_loss: 1.7319 - val_accuracy: 0.4667\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 64s 17s/step - loss: 2.2938 - accuracy: 0.4542 - val_loss: 1.5279 - val_accuracy: 0.5167\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 77s 20s/step - loss: 2.1909 - accuracy: 0.4458 - val_loss: 1.4889 - val_accuracy: 0.5000\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 79s 20s/step - loss: 2.4085 - accuracy: 0.4208 - val_loss: 1.4512 - val_accuracy: 0.5000\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 79s 21s/step - loss: 2.4839 - accuracy: 0.4042 - val_loss: 1.3934 - val_accuracy: 0.5000\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 80s 20s/step - loss: 2.1019 - accuracy: 0.4625 - val_loss: 1.4473 - val_accuracy: 0.5333\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 82s 21s/step - loss: 2.2184 - accuracy: 0.4583 - val_loss: 1.5004 - val_accuracy: 0.5333\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 79s 20s/step - loss: 2.2027 - accuracy: 0.4375 - val_loss: 1.4374 - val_accuracy: 0.5167\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 76s 20s/step - loss: 2.2533 - accuracy: 0.4750 - val_loss: 1.3224 - val_accuracy: 0.4833\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 78s 20s/step - loss: 1.9211 - accuracy: 0.5375 - val_loss: 1.3783 - val_accuracy: 0.5000\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 71s 18s/step - loss: 2.0218 - accuracy: 0.4625 - val_loss: 1.3538 - val_accuracy: 0.4833\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 54s 14s/step - loss: 1.9332 - accuracy: 0.4708 - val_loss: 1.5397 - val_accuracy: 0.4833\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 2.3404 - accuracy: 0.4417 - val_loss: 1.2569 - val_accuracy: 0.5167\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 1.8537 - accuracy: 0.5417 - val_loss: 1.4278 - val_accuracy: 0.4833\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 52s 14s/step - loss: 1.8026 - accuracy: 0.5292 - val_loss: 1.3748 - val_accuracy: 0.5333\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 50s 13s/step - loss: 1.8768 - accuracy: 0.4958 - val_loss: 1.2607 - val_accuracy: 0.5500\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 52s 14s/step - loss: 1.9969 - accuracy: 0.4458 - val_loss: 1.4745 - val_accuracy: 0.4833\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 52s 13s/step - loss: 2.1432 - accuracy: 0.4875 - val_loss: 1.4047 - val_accuracy: 0.5000\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 2.0065 - accuracy: 0.4833 - val_loss: 1.3381 - val_accuracy: 0.5333\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 1.7467 - accuracy: 0.5375 - val_loss: 1.2684 - val_accuracy: 0.5500\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 1.7462 - accuracy: 0.5208 - val_loss: 1.2967 - val_accuracy: 0.5500\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 55s 14s/step - loss: 1.8212 - accuracy: 0.4625 - val_loss: 1.2436 - val_accuracy: 0.5500\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 55s 15s/step - loss: 1.8642 - accuracy: 0.5500 - val_loss: 1.2625 - val_accuracy: 0.5833\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 54s 14s/step - loss: 1.8206 - accuracy: 0.5000 - val_loss: 1.2251 - val_accuracy: 0.5667\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 53s 14s/step - loss: 1.6838 - accuracy: 0.5208 - val_loss: 1.1903 - val_accuracy: 0.5167\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 53s 14s/step - loss: 1.5754 - accuracy: 0.5750 - val_loss: 1.3724 - val_accuracy: 0.5333\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 52s 13s/step - loss: 1.8153 - accuracy: 0.4833 - val_loss: 1.3470 - val_accuracy: 0.5000\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 1.4627 - accuracy: 0.5333 - val_loss: 1.2682 - val_accuracy: 0.5667\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 1.5722 - accuracy: 0.5250 - val_loss: 1.2483 - val_accuracy: 0.5333\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 51s 13s/step - loss: 1.6204 - accuracy: 0.5500 - val_loss: 1.2224 - val_accuracy: 0.5167\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 54s 14s/step - loss: 1.7400 - accuracy: 0.5417 - val_loss: 1.2347 - val_accuracy: 0.5500\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 56s 15s/step - loss: 1.5929 - accuracy: 0.5333 - val_loss: 1.2004 - val_accuracy: 0.5500\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 54s 14s/step - loss: 1.6892 - accuracy: 0.5375 - val_loss: 1.3623 - val_accuracy: 0.4667\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 53s 14s/step - loss: 1.5910 - accuracy: 0.5417 - val_loss: 1.2628 - val_accuracy: 0.5500\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 53s 14s/step - loss: 1.6409 - accuracy: 0.5417 - val_loss: 1.4101 - val_accuracy: 0.5167\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 52s 14s/step - loss: 1.3932 - accuracy: 0.5667 - val_loss: 1.1695 - val_accuracy: 0.5833\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 52s 14s/step - loss: 1.4547 - accuracy: 0.5583 - val_loss: 1.1496 - val_accuracy: 0.5333\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 52s 14s/step - loss: 1.7183 - accuracy: 0.4958 - val_loss: 1.2581 - val_accuracy: 0.5500\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 53s 14s/step - loss: 1.8098 - accuracy: 0.4750 - val_loss: 1.2255 - val_accuracy: 0.6000\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 53s 14s/step - loss: 1.4406 - accuracy: 0.5583 - val_loss: 1.2384 - val_accuracy: 0.5667\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "base_model = VGG16(include_top=False, input_shape=(img_size, img_size, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "predictions = Dense(len(classes), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=20, zoom_range=0.2, shear_range=0.2)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=60), epochs=50, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save('girlFacevgg13.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Prediction probabilities: [[7.3145545e-01 7.0336232e-06 1.6788846e-01 1.0064911e-01]]\n",
            "Predicted class indices: [0]\n",
            "Predicted face shape: oval\n"
          ]
        }
      ],
      "source": [
        "# Load the model and make a prediction\n",
        "model = tf.keras.models.load_model('girlFacevgg13.h5')\n",
        "\n",
        "# Define the image path and load the image\n",
        "img_path = 'C:/Users/user/Downloads/Data settt/Girl/Taining dan testing/Testing/oval/img_no_203.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "# Convert image to array\n",
        "input_img = image.img_to_array(img)\n",
        "input_img = np.expand_dims(input_img, axis=0)\n",
        "input_img = preprocess_input(input_img)\n",
        "\n",
        "# Predict the inputs on the model\n",
        "predict_img = model.predict(input_img)\n",
        "\n",
        "# Debugging: Print prediction probabilities and indices\n",
        "print(f\"Prediction probabilities: {predict_img}\")\n",
        "\n",
        "# Define your list of face shape classes\n",
        "classes = [\"oval\", \"rectangular\", \"round\", \"square\"]\n",
        "\n",
        "# Map predicted class indices to class labels\n",
        "predicted_class_indices = np.argmax(predict_img, axis=1)\n",
        "print(f\"Predicted class indices: {predicted_class_indices}\")\n",
        "\n",
        "# Ensure predicted class indices are within range of classes list\n",
        "predicted_classes = []\n",
        "for idx in predicted_class_indices:\n",
        "    if 0 <= idx < len(classes):\n",
        "        predicted_classes.append(classes[idx])\n",
        "    else:\n",
        "        predicted_classes.append(\"Unknown\")  # Handle out-of-range indices gracefully\n",
        "\n",
        "# Print predicted face shape\n",
        "print(f\"Predicted face shape: {predicted_classes[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmp0fbdj9dg\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmp0fbdj9dg\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model successfully converted to TFLite and saved as 'girlvgg3.tflite'\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the Keras model\n",
        "model = tf.keras.models.load_model('girlFacevgg13.h5')\n",
        "\n",
        "# Convert the model to TFLite with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model to disk\n",
        "with open('girlvgg.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model successfully converted to TFLite and saved as 'girlvgg3.tflite'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
